{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "877b76c0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\daniele\\anaconda3\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\daniele\\anaconda3\\lib\\site-packages (0.14.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\daniele\\anaconda3\\lib\\site-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\daniele\\anaconda3\\lib\\site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\daniele\\anaconda3\\lib\\site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\daniele\\anaconda3\\lib\\site-packages (from torchvision) (1.21.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\daniele\\anaconda3\\lib\\site-packages (from requests->torchvision) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\daniele\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\daniele\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\daniele\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e6e192d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming dataset from absolute values in relative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a5bbd792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "image_width = 676\n",
    "image_height = 380\n",
    "\n",
    "matrix = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1247523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = pd.read_csv(\"data.csv\")\n",
    "\n",
    "matrix['xcenter'] = matrix['xcenter'] / image_width\n",
    "matrix['ycenter'] = matrix['ycenter'] / image_height\n",
    "matrix['width'] = matrix['width'] / image_width\n",
    "matrix['heigth'] = matrix['heigth'] / image_height\n",
    "\n",
    "df = pd.DataFrame(matrix)\n",
    "\n",
    "df.to_csv('relative_dimentions_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4040bfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating train.csv file that has 2 columns: \"image\" -> name of the image, \"label\" -> name of the label txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f380fb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_train_matrix = df\n",
    "dt_train_matrix = dt_train_matrix.drop(columns=[\"xcenter\"])\n",
    "dt_train_matrix = dt_train_matrix.drop(columns=[\"ycenter\"])\n",
    "dt_train_matrix = dt_train_matrix.drop(columns=[\"width\"])\n",
    "dt_train_matrix = dt_train_matrix.drop(columns=[\"heigth\"])\n",
    "dt_train_matrix['txt'] = dt_train_matrix['image']\n",
    "dt_train_matrix['txt'] = dt_train_matrix['txt'].astype(str).replace(\"jpg\", \"txt\", regex=True)\n",
    "dt_train_matrix.to_csv('train.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "db48dbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating all label files and writing inside the values of the label (class, xcenter, ycenter, width, heigth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "30482342",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    with open('labels/'+ row['image'].replace(\".jpg\", \"\") +'.txt', 'w') as file:\n",
    "        file.write('0 ' + str(row['xcenter']) + ' ' + str(row['ycenter']) + ' '  + str(row['width']) + ' '  + str(row['heigth']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac7f110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates a Pytorch dataset to load the Pascal VOC dataset\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class VOCDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self, csv_file, img_dir, label_dir, S=7, B=2, C=1, transform=None,\n",
    "    ):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.S = S #split size\n",
    "        self.B = B #number of bounding boxes\n",
    "        self.C = C\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label_path = os.path.join(self.label_dir, self.annotations.iloc[index, 1])\n",
    "        boxes = []\n",
    "        with open(label_path) as f:\n",
    "            for label in f.readlines():\n",
    "                class_label, x, y, width, height = [\n",
    "                    float(x) if float(x) != int(float(x)) else int(x)\n",
    "                    for x in label.replace(\"\\n\", \"\").split()\n",
    "                ]\n",
    "\n",
    "                boxes.append([class_label, x, y, width, height])\n",
    "\n",
    "        img_path = os.path.join(self.img_dir, self.annotations.iloc[index, 0])\n",
    "        image = Image.open(img_path)\n",
    "        boxes = torch.tensor(boxes)\n",
    "\n",
    "        if self.transform:\n",
    "            # image = self.transform(image)\n",
    "            image, boxes = self.transform(image, boxes)\n",
    "\n",
    "        # Convert To Cells\n",
    "        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n",
    "\n",
    "        for box in boxes:\n",
    "            class_label, x, y, width, height = box.tolist()\n",
    "            class_label = int(class_label)\n",
    "\n",
    "            # i,j represents the cell row and cell column\n",
    "            i, j = int(self.S * y), int(self.S * x)\n",
    "            x_cell, y_cell = self.S * x - j, self.S * y - i\n",
    "\n",
    "            \"\"\"\n",
    "            Calculating the width and height of cell of bounding box,\n",
    "            relative to the cell is done by the following, with\n",
    "            width as the example:\n",
    "            \n",
    "            width_pixels = (width*self.image_width)\n",
    "            cell_pixels = (self.image_width)\n",
    "            \n",
    "            Then to find the width relative to the cell is simply:\n",
    "            width_pixels/cell_pixels, simplification leads to the\n",
    "            formulas below.\n",
    "            \"\"\"\n",
    "            width_cell, height_cell = (\n",
    "                width * self.S,\n",
    "                height * self.S,\n",
    "            )\n",
    "\n",
    "            # If no object already found for specific cell i,j\n",
    "            # Note: This means we restrict to ONE object\n",
    "            # per cell!\n",
    "            if label_matrix[i, j, 1] == 0:\n",
    "                # Set that there exists an object\n",
    "                label_matrix[i, j, 1] = 1\n",
    "\n",
    "                # Box coordinates\n",
    "                box_coordinates = torch.tensor(\n",
    "                    [x_cell, y_cell, width_cell, height_cell]\n",
    "                )\n",
    "\n",
    "                label_matrix[i, j, 2:6] = box_coordinates\n",
    "\n",
    "                # Set one hot encoding for class_label\n",
    "                label_matrix[i, j, class_label] = 1\n",
    "\n",
    "        return image, label_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4532dac4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=676x380>,\n",
       " tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000],\n",
       "          [1.0000, 1.0000, 0.6664, 0.0629, 0.3951, 0.3115, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000]]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = VOCDataset(\"train.csv\", img_dir=\"images/\", label_dir=\"labels/\",)\n",
    "test_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf41ffe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb6f4e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
